---
title: "NOAA Storm Data - Analysis of Significant Factors"
author: "Graham King"
date: "21 December 2014"
output: html_document
---
```{r global_options, include=FALSE}
library(knitr)
opts_chunk$set(fig.width=12)
```

## Synopsis

This report is an analysis of [this dataset](https://d396qusza40orc.cloudfront.net/repdata%2Fdata%2FStormData.csv.bz2) of storm and weather events, published by the U.S. National Oceanic and Atmospheric Administration's (NOAA) storm database, in order to identify the most significant factors in terms of health and economic impact.

The weather event causing most fatalities in the US between 1995 and 2011 is Excessive Heat, closely followed by Tornado. Tornados cause by far the most injuries of any weather event.

In terms of economic cost, Flood has by far the most signficant impact, then Hurricane/Typhoon, followed by Storm Surge and Tornado.

The fine granularity of weather event categories could have an impact on the results. Further analysis of this is recommended.

****

## Data Processing

### Sourcing the Data
```{r cache=TRUE}
filename <- "StormData.csv.bz2"
url <- "https://d396qusza40orc.cloudfront.net/repdata%2Fdata%2FStormData.csv.bz2"

if (!file.exists(filename)) {
    download.file(url=url, destfile = filename, method="curl")
}
raw.data <- read.csv(
    filename, header=TRUE, na.strings=c(""))
```

### First peek at the data
Having downloaded the data and read the file into variable `raw.data`, let's look at its structure:
```{r}
dim(raw.data)
str(raw.data)
```
In summary, this dataset contains 902297 observations of 37 variables.

How complete is the data? Let's look for NAs:
```{r}
nas <- c()
for (i in 1:37) nas[i] <- sum(is.na(raw.data[,i]))
names(nas) <- names(raw.data)
print(nas[nas>0])
```
There are quite a lot of NAs but not in variables I'm interested in for this report. I can handle Property/Crop damage exponents (`PROPDMGEXP` and `CROPDMGEXP`) by treating the associated damage number as an absolute value (i.e. no exponent).

### Data Formatting
Firstly, let's remove columns we're not interested in:
```{r}
dtc <- raw.data[,
                -c(1,3,4,5,6,9,10,11,12,13,14,15,16,17,18,19,20,21,22,
                   29,30,31,32,33,34,35,36,37)]
```
The `BGN_DATE` variable is a factor, so this should be formatted as a Date:
```{r}
dtc[,"BGN_DATE"] <- as.Date(dtc[,"BGN_DATE"], "%m/%d/%Y %H:%M:%S")
dtc$year <- as.POSIXlt(as.Date(dtc[,"BGN_DATE"], "%m/%d/%Y %H:%M:%S"))$year+1900
```

The `EVTYPE` variable contains unnecessary whitespace, let's remove it:
```{r}
library(stringr)
dtc$EVTYPE <- factor(str_trim(dtc$EVTYPE))
```

Assessing property damage requires turning the numerical and exponential variables into a purely numerical value that can be used arithmetically. The values in the `PROPDMGEXP` and `CROPDMGEXP` variables do not correspond only to the 'K' (thousands), 'M' (millions) and 'B' (billions) expected:
```{r}
table(dtc$PROPDMGEXP)
table(dtc$CROPDMGEXP)
```
Since we don't know how to treat many of these values, I'll remove them and use a helper function (`makenum`) to create a new column with absolute numbers (i.e 1B -> 1,000,000,000):
```{r}
source("functions.R")
exp <- c("k", "K", "m", "M", "b", "B", "", NA)
dtc <- dtc[dtc$PROPDMGEXP %in% exp &
           dtc$CROPDMGEXP %in% exp,]
dtc$PROPDMGEXP <- factor(dtc$PROPDMGEXP)
dtc$CROPDMGEXP <- factor(dtc$CROPDMGEXP)
dtc$PROPDMGNUM <- mapply(makenum, dtc$PROPDMG, dtc$PROPDMGEXP)
dtc$CROPDMGNUM <- mapply(makenum, dtc$CROPDMG, dtc$CROPDMGEXP)
```

The source of `functions.R` can be viewed [here](https://github.com/gking2224/RepData_PeerAssessment2/blob/master/functions.R).

The processed data now look like this:
```{r}
str(dtc)
head(dtc)
```

****

## Initial Questions
The following questions were addressed in order to get some familiarity with the data.

1. *How many events were recorded per year?*
```{r}
yd <- as.data.frame(tapply(dtc$year, dtc$year, length))
yd$year <- row.names(yd)
row.names(yd) <- NULL
names(yd) <- c("count", "year")

library(ggplot2)
library(grid)
ye <- ggplot(yd, aes(x=year, y=count))
ye <- ye + geom_bar(stat="identity")
ye <- ye + xlab("Year") + ylab("Observations")
ye <- ye + ggtitle("Figure 1: Total Observations per Year")
ye <- ye + theme(axis.text.x = element_text(angle = 45, hjust = 1))
print(ye)

print(min(dtc$year)); nrow(dtc[dtc$year==min(dtc$year),])
print(max(dtc$year)); nrow(dtc[dtc$year==max(dtc$year),])
```

This shows that there were very few events (`r nrow(dtc[dtc$year==min(dtc$year),])`) recorded in the earliest year (`r min(dtc$year)`) contained in the dataset, rising slowly during the 70s, 80s and early 90s before  suddenly increasing in the mid 90s and then rising more rapidly to a peak of `r nrow(dtc[dtc$year==max(dtc$year),])` in `r max(dtc$year)`

Given the low rate of data collection in the earlier years and the increased relevance (and presumably lower sampling error rate) of later data, I shall limit this analysis to data recorded between 1995 and 2011 (inclusive).
```{r}
dtc <- dtc[dtc$year >=1995,]
```

2. *What are the most frequently occurring event types*
```{r}
by.evtype <- tapply(dtc$year, dtc$EVTYPE, length)
by.evtype <- as.data.frame(by.evtype[order(by.evtype, decreasing=TRUE)])
names(by.evtype) <- "count"
head(by.evtype)
```
The most frequently occuring event over the period is Hail, with nearly ten times as many recorded events as Flood and Tornado.

****

## Results

### Most Harmful Weather Event

Here I group injuries and fatalities data by event type, summing up the number of each per event type. This is then combined into a single health impact dataset and order first by number of fatalities then injuries.
```{r}
byinjuries <- as.data.frame(tapply(dtc$INJURIES, dtc$EVTYPE, sum))
byinjuries$EVTYPE<-row.names(byinjuries)
names(byinjuries) <- c("injuries", "evtype")
byfatalities <- as.data.frame(tapply(dtc$FATALITIES, dtc$EVTYPE, sum))
byfatalities$EVTYPE<-row.names(byfatalities)
names(byfatalities) <- c("fatalities", "evtype")

health <- merge(byinjuries, byfatalities)
health <- health[order(health$fatalities, health$injuries, decreasing = TRUE),]
health <- health[1:10,]
dimnames(health$fatalities) <- NULL
dimnames(health$injuries) <- NULL
health$evtype <- factor(health$evtype)

library(reshape2)
hm <- melt(health, measure.vars=c("fatalities", "injuries"),
           id.vars="evtype", variable.name="type", value.name="count")
dim(hm$count) <- NULL

gp <- ggplot(hm, aes(x=evtype, y=count))
gp <- gp + geom_bar(stat="identity")
gp <- gp + facet_grid(type ~ ., scales="free_y",
                      labeller=function(x,y){
                          ifelse(y=="fatalities","Fatalities", "Injuries")})
gp <- gp + xlab("Weather Event Type") + ylab("Incidence (total in the period)")
gp <- gp + ggtitle("Figure 2: Health impact of US weather events (1995-2011)")
gp <- gp + theme(axis.text.x = element_text(angle = 45, hjust = 1))

print(gp)
```

This figure plots the fatalities and injuries caused by the top 10 weather event types. It shows that Excessive Heat causes the most fatalities, followed by Tornado. Tornados cause th emost injuries by a significant margin.

*Note that a limitation of the way that this data was sorted is that a weather event with a large number of injuries but few fatalities may not register.*

****

### Most Economically Damaging Weather Event

Create a new variable `TOTALDMG` which is the sum of the `CROPDMGNUM` and `PROPDMGNUM` variables. Then group the data by event type, recording the sum of `TOTALDMG` of all observations of that weather event type.

```{r}

dtc$TOTALDMG <- dtc$CROPDMGNUM + dtc$PROPDMGNUM
bydmg <- as.data.frame(tapply(dtc$TOTALDMG, dtc$EVTYPE, sum))
bydmg$EVTYPE<-row.names(bydmg)
bydmg <- bydmg[order(bydmg[1], decreasing=TRUE),]
names(bydmg) <- c("cost", "evtype")

bn <- 10^9
scale <- signif(max(bydmg$cost, na.rm=TRUE), 1) / bn

library(ggplot2)
gc <- ggplot(bydmg[1:10,], aes(x=factor(evtype), y=cost))
gc <- gc + geom_bar(stat="identity")
gc <- gc + xlab("Event Type") + ylab("Total Cost ($)")
gc <- gc + ggtitle("Figure 3: Total cost of US weather events (1995-2011)")
gc <- gc + scale_y_continuous(
    breaks=bn * seq(0,150, by=25),
    labels=paste0(seq(0,150, by=25), "bn"),
    limits=bn * c(0,155)
)
gc <- gc + theme(axis.text.x = element_text(angle = 45, hjust = 1))
print(gc)
```

This figure shows the total cost of the top ten weather event types. It shows that the most costly type of weather event is Flood, followed by Hurricane/Typhoon, then Storm Surge and Tornado.

The results indicate that the granularity of weather event categorisation could affect the stated impact of certain types of weather event. For instance, perhaps the results would be different if High Wind, Hurricane, Hurricane/Typhoon, Tornado & Tropical Storm were all treated as the same type of event.

****

## Appendix 1 - Session Info
This report was produced on the following hardware setup:

Attribute | Detail
-- | --
OS | OS X Yosemite 10.10.1
Processor | 3.4 GHz Intel Core i7
Memory | 16GB 1333 MHz DDR3
Graphics | AMD Radeon HD 6970M 2048 MB

The R environment is as follows:
```{r}
sessionInfo()
```
R Studio version used is:
```
Version 0.98.1091 – © 2009-2014 RStudio, Inc.
Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/600.2.5 (KHTML, like Gecko)
```